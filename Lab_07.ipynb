{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\seema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\seema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\seema\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\seema\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Autumn leaves scatter across the park, their vibrant hues signaling the arrival of cooler days. Children play among the foliage, their laughter filling the air. A gentle breeze rustles the trees, while parents watch from nearby benches. It's a season of change and joy, inviting everyone outdoors to enjoy nature's beauty.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autumn', 'leaves', 'scatter', 'across', 'the', 'park', ',', 'their', 'vibrant', 'hues', 'signaling', 'the', 'arrival', 'of', 'cooler', 'days', '.', 'Children', 'play', 'among', 'the', 'foliage', ',', 'their', 'laughter', 'filling', 'the', 'air', '.', 'A', 'gentle', 'breeze', 'rustles', 'the', 'trees', ',', 'while', 'parents', 'watch', 'from', 'nearby', 'benches', '.', 'It', \"'s\", 'a', 'season', 'of', 'change', 'and', 'joy', ',', 'inviting', 'everyone', 'outdoors', 'to', 'enjoy', 'nature', \"'s\", 'beauty', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autumn leaves scatter across the park, their vibrant hues signaling the arrival of cooler days.', 'Children play among the foliage, their laughter filling the air.', 'A gentle breeze rustles the trees, while parents watch from nearby benches.', \"It's a season of change and joy, inviting everyone outdoors to enjoy nature's beauty.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tag = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Autumn', 'NNP'), ('leaves', 'VBZ'), ('scatter', 'JJR'), ('across', 'IN'), ('the', 'DT'), ('park', 'NN'), (',', ','), ('their', 'PRP$'), ('vibrant', 'JJ'), ('hues', 'NNS'), ('signaling', 'VBG'), ('the', 'DT'), ('arrival', 'NN'), ('of', 'IN'), ('cooler', 'NN'), ('days', 'NNS'), ('.', '.'), ('Children', 'NNP'), ('play', 'NN'), ('among', 'IN'), ('the', 'DT'), ('foliage', 'NN'), (',', ','), ('their', 'PRP$'), ('laughter', 'NN'), ('filling', 'VBG'), ('the', 'DT'), ('air', 'NN'), ('.', '.'), ('A', 'DT'), ('gentle', 'JJ'), ('breeze', 'NN'), ('rustles', 'VBZ'), ('the', 'DT'), ('trees', 'NNS'), (',', ','), ('while', 'IN'), ('parents', 'NNS'), ('watch', 'VBP'), ('from', 'IN'), ('nearby', 'JJ'), ('benches', 'NNS'), ('.', '.'), ('It', 'PRP'), (\"'s\", 'VBZ'), ('a', 'DT'), ('season', 'NN'), ('of', 'IN'), ('change', 'NN'), ('and', 'CC'), ('joy', 'NN'), (',', ','), ('inviting', 'VBG'), ('everyone', 'NN'), ('outdoors', 'NNS'), ('to', 'TO'), ('enjoy', 'VB'), ('nature', 'NN'), (\"'s\", 'POS'), ('beauty', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(to_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'if', 'after', 'itself', 'between', 'couldn', \"needn't\", 'was', \"mustn't\", \"mightn't\", 'ourselves', 'ma', 'because', 'doesn', 'that', 'is', 'her', 'theirs', 'few', 'himself', 'wouldn', 'more', 'we', 'isn', \"you've\", 'below', 'didn', 'how', 'this', 'their', 'should', 'y', 'but', \"won't\", 'through', 'same', \"shouldn't\", 'my', 'for', 'being', 'shouldn', 'again', 'of', \"isn't\", 'me', 'needn', 'the', 'by', 'own', 'have', 'both', \"you'd\", 'there', 'about', 'nor', 'very', 'what', 'up', 'your', \"wasn't\", 'on', 'under', 'ours', 's', 'his', 'why', 'just', 'over', 'hers', 'who', 'myself', 'won', \"should've\", 'other', 'during', \"she's\", 't', 'hasn', 'its', \"you're\", 'where', 'are', 'our', 'not', 'he', 'before', 'so', \"hadn't\", \"don't\", 'only', 'him', 'an', 'yourselves', 'these', \"couldn't\", 'don', 'above', 'against', 'those', 'any', 'i', 'can', 'hadn', \"didn't\", 'm', \"that'll\", 'yours', 'ain', 'until', 'when', 'be', \"it's\", 'do', 'haven', 'a', \"aren't\", \"shan't\", 'o', 'out', 'each', 'whom', 'you', 'they', 'from', 'll', 'some', 'had', \"haven't\", 'which', 'here', 'with', 'further', 'yourself', 'shan', 'herself', \"doesn't\", 'off', 'has', 'then', \"weren't\", 'will', 'into', 'or', \"you'll\", 'now', 'and', 're', 'as', 'in', 'been', 'am', 'while', 'once', 'doing', 'down', 'did', 'themselves', 'such', 'mightn', 'weren', 'no', 've', 'it', 'them', 'at', 'she', 'all', 'd', \"hasn't\", \"wouldn't\", 'most', 'than', 'aren', 'were', 'to', 'wasn', 'too', 'having', 'mustn', 'does'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_clean = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Autumn', 'leaves', 'scatter', 'across', 'park', ',', 'vibrant', 'hues', 'signaling', 'arrival', 'cooler', 'days', '.', 'Children', 'play', 'among', 'foliage', ',', 'laughter', 'filling', 'air', '.', 'A', 'gentle', 'breeze', 'rustles', 'trees', ',', 'parents', 'watch', 'nearby', 'benches', '.', 'It', \"'s\", 'season', 'change', 'joy', ',', 'inviting', 'everyone', 'outdoors', 'enjoy', 'nature', \"'s\", 'beauty', '.']\n"
     ]
    }
   ],
   "source": [
    "no_stopwords_text = []\n",
    "for token in to_clean:\n",
    "    if(token not in stop_words):\n",
    "        no_stopwords_text.append(token)\n",
    "\n",
    "print(no_stopwords_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = []\n",
    "for token in no_stopwords_text:\n",
    "    stemmed_word = stemmer.stem(token)\n",
    "    stemmed_words.append(stemmed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipisc', 'elit', '.', 'fusc', 'commodo', 'mauri', 'id', 'justo', 'condimentum', 'dignissim', '.', 'nullam', 'placerat', 'semper', 'dapibu', '.', 'pellentesqu', 'ac', 'risu', 'nulla', '.', 'phasellu', 'ut', 'dapibu', 'nunc', ',', 'id', 'aliquam', 'dolor', '.']\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = []\n",
    "for token in no_stopwords_text:\n",
    "    lemmatized = lemmatizer.lemmatize(token)  # Assuming you want to lemmatize verbs (you can change the 'pos' argument as needed)\n",
    "    lemmatized_words.append(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorem', 'ipsum', 'dolor', 'sit', 'amet', ',', 'consectetur', 'adipiscing', 'elit', '.', 'Fusce', 'commodo', 'mauris', 'id', 'justo', 'condimentum', 'dignissim', '.', 'Nullam', 'placerat', 'semper', 'dapibus', '.', 'Pellentesque', 'ac', 'risus', 'nulla', '.', 'Phasellus', 'ut', 'dapibus', 'nunc', ',', 'id', 'aliquam', 'dolor', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love to eat pizza\",\n",
    "    \"Pizza is my favorite food\",\n",
    "    \"I enjoy eating pizza with friends\",\n",
    "    \"I like to have pizza for dinner\",\n",
    "    \"Pizza toppings include cheese, pepperoni, and mushrooms\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.58946308 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.58946308 0.         0.         0.\n",
      "  0.28088232 0.4755751  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.48638585 0.48638585 0.         0.         0.         0.\n",
      "  0.48638585 0.         0.         0.         0.48638585 0.\n",
      "  0.23176546 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.48638585 0.48638585\n",
      "  0.         0.         0.         0.48638585 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23176546 0.         0.         0.48638585]\n",
      " [0.         0.         0.45277275 0.         0.         0.\n",
      "  0.         0.         0.45277275 0.         0.45277275 0.\n",
      "  0.         0.45277275 0.         0.         0.         0.\n",
      "  0.21574864 0.36529421 0.         0.        ]\n",
      " [0.40073619 0.40073619 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.40073619\n",
      "  0.         0.         0.         0.40073619 0.         0.40073619\n",
      "  0.19095294 0.         0.40073619 0.        ]]\n",
      "['and' 'cheese' 'dinner' 'eat' 'eating' 'enjoy' 'favorite' 'food' 'for'\n",
      " 'friends' 'have' 'include' 'is' 'like' 'love' 'mushrooms' 'my'\n",
      " 'pepperoni' 'pizza' 'to' 'toppings' 'with']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
